{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Задание к 4.4.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyNpYoOWFFAfihXS2nm8crsd"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"xZzM7bk053G4"},"source":["Как было сказано в предыдущем уроке, полносвязный слой может быть представлен как матричное умножение матрицы входов (X) и матрицы весов нейронов слоя (W), плюс вектор bias'ов слоя (b). \r\n","\r\n","В документации к классу torch.nn.Linear (полносвязному слою) написано следующее: Applies a linear transformation to the incoming data: y = x * A^T + b. А здесь – это то, как PyTorch хранит веса слоя. Но чтобы эта матрица совпала с W из предыдущего урока, нужно её сперва транспонировать.\r\n","\r\n","\r\n","Давайте реализуем функциональность torch.nn.Linear и сверим с оригиналом!\r\n","\r\n","\r\n","Пусть у нас будет 1 объект x на входе с двумя компонентами. Его мы передадим в полносвязный слой с 3-мя нейронами и получим, соотсветственно, 3 выхода. После напишем эту же функциональность с помощью матричного умножения. "]},{"cell_type":"code","metadata":{"id":"zRkNyPaVcOHp","executionInfo":{"status":"ok","timestamp":1611399795016,"user_tz":-180,"elapsed":594,"user":{"displayName":"Oleg Nai","photoUrl":"","userId":"08793324282693556978"}}},"source":["import torch\r\n","\r\n","# Сперва создадим тензор x:\r\n","x = torch.tensor([[10., 20.]])\r\n","\r\n","# Оригинальный полносвязный слой с 2-мя входами и 3-мя нейронами (выходами):\r\n","fc = torch.nn.Linear(2, 3)\r\n","\r\n","# Веса fc-слоя хранятся в fc.weight, а bias'ы соответственно в fc.bias\r\n","# fc.weight и fc.bias по умолчанию инициализируются случайными числами\r\n","\r\n","# Давайте проставим свои значения в веса и bias'ы:\r\n","w = torch.tensor([[11., 12.], [21., 22.], [31., 32]])\r\n","fc.weight.data = w\r\n","\r\n","b = torch.tensor([[31., 32., 33.]])\r\n","fc.bias.data = b\r\n","\r\n","# Получим выход fc-слоя:\r\n","fc_out = fc(x)\r\n","\r\n","# Попробуем теперь получить аналогичные выходы с помощью матричного перемножения:\r\n","fc_out_alternative = torch.add(torch.mm(x,torch.t(w)), b) \r\n","\r\n","# Проверка осуществляется автоматически вызовом функции\r\n","#print(fc_out == fc_out_alternative)\r\n","# (раскомментируйте, если решаете задачу локально)"],"execution_count":74,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fdo2RCCJ6SsN"},"source":["В предыдущем шаге мы написали функцию, эмулирующую fc-слой. Проверим, что по ней правильно считается градиент. \r\n","\r\n","Функцию backward() в PyTorch можно посчитать только от скалярной функции (выход из такой функции – одно число). Это логично, так как loss-функция выдает всегда одно число. Но fc-слой, который мы проэмулировали, имел 3 выхода. Предлагаем их просуммировать, чтобы получить в итоге скалярную функцию. Заметим, впрочем, что можно было бы выбрать любую агрегирующую операцию, например умножение.\r\n","\r\n","Дополните код так, чтобы градиент по весам и смещениям (bias) совпадал с аналогичным градиентом в вашей фунции.\r\n","\r\n","Чем обусловлен полученный градиент? Изменится ли он, если мы подадим другие входы или другую инициализацию весов?"]},{"cell_type":"code","metadata":{"id":"cXNhW62U6W8H","executionInfo":{"status":"ok","timestamp":1611399792609,"user_tz":-180,"elapsed":660,"user":{"displayName":"Oleg Nai","photoUrl":"","userId":"08793324282693556978"}}},"source":["import torch\r\n","\r\n","# Сперва создадим тензор x:\r\n","x = torch.tensor([[10., 20.]])\r\n","\r\n","# Оригинальный полносвязный слой с 2-мя входами и 3-мя нейронами (выходами):\r\n","fc = torch.nn.Linear(2, 3)\r\n","\r\n","# Веса fc-слоя хранятся в fc.weight, а bias'ы соответственно в fc.bias\r\n","# fc.weight и fc.bias по умолчанию инициализируются случайными числами\r\n","\r\n","# Давайте проставим свои значения в веса и bias'ы:\r\n","w = torch.tensor([[11., 12.], [21., 22.], [31., 32]])\r\n","fc.weight.data = w\r\n","\r\n","b = torch.tensor([[31., 32., 33.]])\r\n","fc.bias.data = b\r\n","\r\n","# Получим выход fc-слоя:\r\n","fc_out = fc(x)\r\n","# Просуммируем выход fc-слоя, чтобы получить скаляр:\r\n","fc_out_summed = fc_out.sum()\r\n","\r\n","# Посчитаем градиенты формулы fc_out_summed:\r\n","fc_out_summed.backward()\r\n","weight_grad = fc.weight.grad\r\n","bias_grad = fc.bias.grad\r\n","\r\n","# Ok, теперь воспроизведем вычисления выше но без fc-слоя:\r\n","# Проставим, что у \"w\" и \"b\" нужно вычислять градиенты (для fc-слоя это произошло автоматически):\r\n","w.requires_grad_(True)\r\n","b.requires_grad_(True)\r\n","\r\n","# Получим выход нашей формулы:\r\n","our_formula =  torch.sum(torch.add(torch.mm(x,torch.t(w)), b)) # SUM{x * w^T + b}\r\n","\r\n","# Сделайте backward для нашей формулы:\r\n","our_formula.backward()\r\n","\r\n","# Проверка осуществляется автоматически, вызовом функций:\r\n","#print('fc_weight_grad:', weight_grad)\r\n","#print('our_weight_grad:', w.grad)\r\n","#print('fc_bias_grad:', bias_grad)\r\n","#print('out_bias_grad:', b.grad)\r\n","# (раскомментируйте, если работаете над задачей локально)"],"execution_count":73,"outputs":[]}]}